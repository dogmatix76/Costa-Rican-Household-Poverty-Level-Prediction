{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# Project: Costa Rican Household Poverty Level Prediction\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction</a></li>\n<li><a href=\"#wrangling\">Data Wrangling</a></li>\n<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n<li><a href=\"#feature_engineering\">Feature Engineering</a></li>\n</ul>"
    },
    {
      "metadata": {
        "_uuid": "85d8d05cc60d88e0350dbbd4010d1a91e71aabd0"
      },
      "cell_type": "markdown",
      "source": "<a id='intro'></a>\n# Introduction\n\n> The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\n> Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.\n\n> In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\n> While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\nBeyond Costa Rica, many countries face this same problem of inaccurately assessing social need.\n\n"
    },
    {
      "metadata": {
        "_uuid": "97ea4e9987a0e15711c4eb040c779e6d32a45b88"
      },
      "cell_type": "markdown",
      "source": "## Problem and Data Explanation\n> The data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\nThis is a supervised multi-class classification machine learning problem:\n- Supervised: provided with the labels for the training data\n- Multi-class classification: Labels are discrete values with 4 classes"
    },
    {
      "metadata": {
        "_uuid": "86f66d7496dd146a5c2c6489cc35e6a7176150a7"
      },
      "cell_type": "markdown",
      "source": "### Objective\n> The objective is to predict poverty on a household level. We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. Moreover, we have to make a prediction for every individual in the test set, but \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis."
    },
    {
      "metadata": {
        "_uuid": "6f0332bb8439f8555d64d25fc2685b6e8f06e130"
      },
      "cell_type": "markdown",
      "source": "The Target values represent poverty levels as follows:\n\n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\n\nThe explanations for all 143 columns can be found in the [competition documentation](https://www.kaggle.com/c/costa-rican-household-poverty-prediction) , but a few to note are below:\n\n- **Id** : a unique identifier for each individual, this should not be a feature that we use!\n- **idhogar** : a unique identifier for each household. This variable is not a feature, but will be used to group individuals by - household as all individuals in a household will have the same identifier.\n- **parentesco1** : indicates if this person is the head of the household.\n- **Target** : the label, which should be equal for all members in a household\n\nWhen we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. These issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job!\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import modules\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a392b8eb93d3368b77e98cdb7db19431efb3c94"
      },
      "cell_type": "code",
      "source": "# Read the data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# To display maximum number of columns \npd.options.display.max_columns = 150",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3a21cf26632439e9a1477403fa3614516f2d119"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1738ffd9f4b6626b679cebaf21ff66e760618871"
      },
      "cell_type": "markdown",
      "source": "<a id='wrangling'></a>\n# Data Wrangling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a36169999ea30a368ab5a771d6acecf30e91da4"
      },
      "cell_type": "code",
      "source": "# Shape of train and test data\nprint(\"Train:\",train.shape)\nprint(\"Test:\",test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8256327baffe4b6c8b591049913e1adfbfa34a0a"
      },
      "cell_type": "code",
      "source": "# To get the information of the data \ntrain.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a7eb38eca8044d0b38bb160d8430bd85a9c7c10"
      },
      "cell_type": "markdown",
      "source": "This tells us there are 130 integer columns, 8 float (numeric) columns, and 5 object columns. The integer columns probably represent Boolean variables (that take on either 0 or 1) or ordinal variables with discrete ordered values. The object columns might pose an issue because they cannot be fed directly into a machine learning model.\n\nLet's glance at the test data which has many more rows (individuals) than the train. It does have one fewer column because there's no Target!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b58375e8243d7e133d273bce73bc3ea32ac1908"
      },
      "cell_type": "code",
      "source": "test.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d3d962b9f2042c998c01cdd5e3f6f4959bad749"
      },
      "cell_type": "markdown",
      "source": " ### Null and missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6473fa58f1cbb156a35f0cc3da7ec79ed2cfe7d"
      },
      "cell_type": "code",
      "source": "# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "436246aeab04372fcb420bebaeb023895f46f63e"
      },
      "cell_type": "markdown",
      "source": "Target missing value is due to joining the test data.  Number of tablets household owns(v18q1) , Monthly rent payment(v2a1), Years behind in school(rez_esc) have most missing values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a84e1792598e21b5da2964484f5a1b75390afb2"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17626994da2928701357942669934996f00a94ab"
      },
      "cell_type": "markdown",
      "source": "<a id='eda'></a>\n# Exploratory Data Analysis"
    },
    {
      "metadata": {
        "_uuid": "ebcd68110e3a5736584fdb7b95b32b365093587f"
      },
      "cell_type": "markdown",
      "source": "### Integer Columns\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acc069eb150a562566aaea6c25a1c7af249f7c5b"
      },
      "cell_type": "code",
      "source": "train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72c5e1646063bbde2e6a336c25bd90fa9c39d8e4"
      },
      "cell_type": "markdown",
      "source": "The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the refrig column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will not need to aggregate these. However, the Boolean columns that are on the individual level will need to be aggregated.\n\n"
    },
    {
      "metadata": {
        "_uuid": "1223392932098df180872bb1a4a0e9a325e86df1"
      },
      "cell_type": "markdown",
      "source": "### Float Columns\nAnother column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an OrderedDict to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n\nThe following graphs shows the distributions of the float columns colored by the value of the Target. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "203c5cb8351275f43538d1af53d2ecd9d05f9d82"
      },
      "cell_type": "code",
      "source": "from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7609bd0ec6f8fdea3b0630b810841f655c6b400e"
      },
      "cell_type": "markdown",
      "source": "Later on we'll calculate correlations between the variables and the Target to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most \"relevant\" to a model. For example, the  meaneduc, representing the average education of the adults in the household appears to be related to the poverty level: a higher average adult education leads to higher values of the target which are less severe levels of poverty. The theme of the importance of education is one we will come back to again and again in this notebook!\n\n- We can see that non vulnarable have more numbers of years of education\n-  We can see that there are less number of people living in one room in the non vulnarable catogery and it increases as the poverty increases"
    },
    {
      "metadata": {
        "_uuid": "d4436d946a09f2df1a7f6e44d1aec0fe66644880"
      },
      "cell_type": "markdown",
      "source": "### Object Columns"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1714dadcbf3e4150e307d28bdde1882ce2fef5f"
      },
      "cell_type": "code",
      "source": "train.select_dtypes('object').head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aaf76c4a7d574fb5d7a11520732500c20cd1f854"
      },
      "cell_type": "markdown",
      "source": "The Id and idhogar object types make sense because these are identifying variables. However, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:\n\n- dependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n- edjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n- edjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\nThese explanations clear up the issue. For these three variables, \"yes\" = 1 and \"no\" = 0. We can correct the variables using a mapping and convert to floats."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbb939573db847ecc12be242034dfe866c0f91eb"
      },
      "cell_type": "code",
      "source": "mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bc1a6a6a90220a902d2852531adf997b27807f5"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ad72985664121b7916af7735e47f2a37572b88d"
      },
      "cell_type": "markdown",
      "source": "Here we can see that\n- years of education of male and female head of household is very low in extreme poverty and very high in non vulnerable.\n- dependency is very low in non vulnareable.\n\nThese variables are now correctly represented as numbers and can be fed into a machine learning model."
    },
    {
      "metadata": {
        "_uuid": "2a0b3b720e48bb2fcb55cf438b01d8f1989931ec"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "7783f90a6ea8a43972ac3aba8ceb029c07ff88eb"
      },
      "cell_type": "markdown",
      "source": "### Join Train and Test Set\nTo make operations like that above a little easier, we'll join together the training and testing dataframes. This is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. Later we can separate out the sets based on the Target."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70a1ccb5e686af7d7eaec10cc5ad9af22ba442a3"
      },
      "cell_type": "code",
      "source": "# Add null Target column to test\ntest['Target'] = np.nan\ntrain_len = len(train)\ndata =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d95589b6d5b5eb5be865d7e3ced6c90e66bf05b4"
      },
      "cell_type": "markdown",
      "source": "## Exploring Label Distribution\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where parentesco1 == 1 because this is the head of household, the correct label for each household.\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f8bdad853024f1b59c9ddebf129e785e6dc5cae"
      },
      "cell_type": "code",
      "source": "# Heads of household\nheads = data.loc[data['parentesco1'] == 1].copy()\n\n# Labels for training\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n\n# Value counts of target\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\n# Bar plot of occurrences of each label\nlabel_counts.plot.bar(figsize = (8, 6), \n                      color = colors.values(),\n                      edgecolor = 'k', linewidth = 2)\n\n# Formatting\nplt.xlabel('Poverty Level'); plt.ylabel('Count'); \nplt.xticks([x - 1 for x in poverty_mapping.keys()], \n           list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown');\n\nlabel_counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ac73392e589475c0ac9ca563ad78bdbb51c0b6f"
      },
      "cell_type": "markdown",
      "source": "We are dealing with an imbalanced class problem. There are many more households that classify as non vulnerable than in any other category. The extreme poverty class is the smallest (I guess this should make us optimistic!).\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. Think about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. One potential method to address class imbalanceds is through oversampling (which is covered in more advanced notebooks)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88107c3940068d24ac9b74b36cedd875ae13241c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f5e3a0f9b57ebfa7b243d0f32bf22d175d50fbb"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c785e90e7953c780fcf8cebff7bf35a082c337c8"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56eb663cad3611b7fadc36d6a6bbcdb143b0cbfe"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d3a9a071352127fb016efb22dc42c06ec5250b2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b87eb32f6987bc44ebe990ecc119497fe6a57f0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25eae0fa61882213617da490e64eeb893d04e2c2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}